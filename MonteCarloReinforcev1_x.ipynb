{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x5icM3dQjnt",
        "outputId": "e30ca3c2-5228-4f4a-9e64-d21af282e659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchinfo\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install swig\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet==1.5.1"
      ],
      "metadata": {
        "id": "bIOECcFvRbML"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "SQp3S49vRbPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import abc\n",
        "from abc import ABC, abstractmethod\n",
        "import typing\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "from itertools import count\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "QC9aweeZR9VW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "DkYlsOy6SCE0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write me line to check the device for cpu pytorch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st9QMpDOoTKP",
        "outputId": "57b270bc-acbc-4b9b-b458-c7c662ad4d36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dims, num_actions, hidden_units):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.num_actions = num_actions\n",
        "    self.input_dims = input_dims\n",
        "    self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self.fc_layer_one = nn.Linear(in_features=self.input_dims, out_features=hidden_units)\n",
        "    self.fc_layer_two = nn.Linear(in_features=hidden_units, out_features=self.num_actions)\n",
        "   # self._init_weights()\n",
        "\n",
        "  # def _init_weights(self):\n",
        "  #   nn.init.zeros_(self.fc_layer_one.weight)\n",
        "  #   nn.init.zeros_(self.fc_layer_two.weight)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = F.relu(self.fc_layer_one(input))\n",
        "    x = self.fc_layer_two(x)\n",
        "    return F.softmax(x,dim=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bZ7g2SnRSEvs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Net Test Area"
      ],
      "metadata": {
        "id": "m8XEQVBa0ZZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "\n",
        "# Reset the environment to get the initial state\n",
        "state = env.reset()\n",
        "num_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "print(\"Initial state:\", state)\n",
        "print(\"Number of Actions:\", num_actions)\n",
        "print(\"State dimension:\", state_dim)\n",
        "\n",
        "learner = NeuralNet(state_dim,num_actions,16)\n",
        "state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "print(f\"Input Tensor shape: {state_tensor.shape}\")\n",
        "#Learner Output\n",
        "out = learner(state_tensor)\n",
        "\n",
        "\n",
        "#print(f\"Output Tensor shape: {out.shape}\")\n",
        "print(f\"Ouput from the Net: {out}\")\n",
        "\n",
        "\n",
        "# Optionally, you can take an action and observe the next state and reward\n",
        "# action = env.action_space.sample()  # Sample a random action\n",
        "# next_state, reward, done, info = env.step(action)\n",
        "\n",
        "# print(\"Next state:\", next_state)\n",
        "# print(\"Reward:\", reward)\n",
        "# print(\"Done:\", done)\n",
        "# print(\"Info:\", info)\n",
        "\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JbAm_bo0YlG",
        "outputId": "5c6b9dff-58d9-419f-99ab-65a3d14a036b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [0.03377339 0.03123699 0.03177733 0.01959691]\n",
            "Number of Actions: 2\n",
            "State dimension: 4\n",
            "Input Tensor shape: torch.Size([1, 4])\n",
            "Ouput from the Net: tensor([[0.4674, 0.5326]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "  def __init__(self, state_dim: int, num_actions: int, hidden_units: int):\n",
        "    self._input_dim = state_dim\n",
        "    self._num_actions = num_actions\n",
        "    self._hidden_units = hidden_units\n",
        "    self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    self._network = NeuralNet(self._input_dim, self._num_actions, self._hidden_units)\n",
        "    self._network.to(self.device)\n",
        "\n",
        "  def get_network(self):\n",
        "    return self._network\n",
        "\n",
        "  def act(self, state: np.ndarray):\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "    probs = self._network(state_tensor).cpu()\n",
        "\n",
        "    action_distribution = Categorical(probs)\n",
        "\n",
        "    action_sample = action_distribution.sample()\n",
        "\n",
        "    #log_prob = action_distribution.log_prob(action_sample)\n",
        "\n",
        "    return action_sample.item(), action_distribution.log_prob(action_sample)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i-J6cxkQ0Yoa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Policy Test Area\n"
      ],
      "metadata": {
        "id": "mAEcTtAIt88r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "\n",
        "# Reset the environment to get the initial state\n",
        "state = env.reset()\n",
        "num_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "print(\"Initial state:\", state)\n",
        "print(\"Number of Actions:\", num_actions)\n",
        "print(\"State dimension:\", state_dim)\n",
        "\n",
        "policy = Policy(state_dim,num_actions,128)\n",
        "state_tensor = torch.tensor(state).unsqueeze(0)\n",
        "print(f\"Input Tensor shape: {state_tensor.shape}\")\n",
        "#Learner Output\n",
        "outa, logout = policy.act(state)\n",
        "\n",
        "#print(f\"Output Tensor shape: {out.shape}\")\n",
        "print(f\"first :Ouput from the Net - Action : {outa}\")\n",
        "print(f\"first :Ouput from the Net - Log Prob : {type(logout)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can take an action and observe the next state and reward\n",
        "# action = env.action_space.sample()  # Sample a random action\n",
        "# next_state, reward, done, info = env.step(action)\n",
        "\n",
        "# print(\"Next state:\", next_state)\n",
        "# print(\"Reward:\", reward)\n",
        "# print(\"Done:\", done)\n",
        "# print(\"Info:\", info)\n",
        "\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGiLhMFQvqfz",
        "outputId": "5f21a056-4f64-4c6e-b0c9-d6abe9bf945e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: [-0.01476944 -0.00631044  0.03936973 -0.01567288]\n",
            "Number of Actions: 2\n",
            "State dimension: 4\n",
            "Input Tensor shape: torch.Size([1, 4])\n",
            "first :Ouput from the Net - Action : 1\n",
            "first :Ouput from the Net - Log Prob : <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FDbaENURe2Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentReinforce:\n",
        "  def __init__(self, env: gym.Env, step_size: float, gamma: float):\n",
        "    self._env = env\n",
        "    self._num_actions = env.action_space.n\n",
        "    self._state_dim = env.observation_space.shape[0]\n",
        "    self._policy = Policy(self._state_dim, self._num_actions,16)\n",
        "    self._network = self._policy.get_network()\n",
        "    self._optimizer = optim.Adam(self._network.parameters(), lr=step_size)\n",
        "    self._gamma = gamma\n",
        "    self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "  def _update_agent(self, action_log_probs: list, expected_return: np.ndarray):\n",
        "    #policy_loss = -torch.sum(action_log_probs * expected_return)\n",
        "    policy_loss = []\n",
        "    for action_probs, discounted_return in zip(action_log_probs, expected_return):\n",
        "      policy_loss.append(action_probs* discounted_return * (-1))\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compute the policy loss using element-wise multiplication and sum along the new dimension\n",
        "\n",
        "    self._optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    self._optimizer.step()\n",
        "\n",
        "  def _compute_discounted_reward(self, rewards_array: list, gamma: float):\n",
        "\n",
        "    num_trajec = len(rewards_array)\n",
        "    discounted_return = deque(maxlen=num_trajec)\n",
        "    #set final discounted return to zero; for ease computation\n",
        "    discounted_return.append(0)\n",
        "    # traverse backwards through the rewards array to compute..\n",
        "    # the expected return at the current time step\n",
        "    for reward in reversed(rewards_array):\n",
        "      #expected return at the current time step\n",
        "      current_expectation = (gamma * discounted_return[0]) + reward\n",
        "      #latest discount return always at the front of the queue and oldest return at the back of the queue\n",
        "      discounted_return.appendleft(current_expectation)\n",
        "\n",
        "    return np.array(discounted_return)\n",
        "\n",
        "  def _generate_trajectories(self):\n",
        "    states = []\n",
        "    actions = []\n",
        "    action_log_probs = []\n",
        "    rewards =[]\n",
        "    done = False\n",
        "    state = self._env.reset()\n",
        "    while not done:\n",
        "      action, action_log_prob = self._policy.act(state)\n",
        "      states.append(state)\n",
        "      actions.append(action)\n",
        "      action_log_probs.append(action_log_prob)\n",
        "      state, reward, done, _, _ = self._env.step(action)\n",
        "      rewards.append(reward)\n",
        "      if np.sum(rewards) > 500:\n",
        "        done = True\n",
        "\n",
        "    return states, rewards, actions, action_log_probs\n",
        "\n",
        "  def _process_trajectories(self, states: list, actions: list, rewards: list):\n",
        "    states_array = np.array(states)\n",
        "    actions_array = np.array(actions)\n",
        "    rewards_array = np.array(rewards)\n",
        "    return states_array, actions_array, rewards_array\n",
        "\n",
        "  def train(self, training_episodes: int):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for episode in range(training_episodes):\n",
        "      states, rewards, actions, action_log = self._generate_trajectories()\n",
        "      #print(f\"Shape of Action Log Prob: {action_log.shape}\")\n",
        "      states_array, actions_array, rewards_array = self._process_trajectories(states,actions,rewards)\n",
        "      scores_deque.append(np.sum(rewards_array))\n",
        "      scores.append(np.sum(rewards_array))\n",
        "      discounted_return = self._compute_discounted_reward(rewards,self._gamma)\n",
        "      discounted_return = torch.tensor(discounted_return, requires_grad=True)\n",
        "      #print(f\"Shape of Discounted return: {discounted_return.shape}\")\n",
        "      #computed discounted rewards are normalized to stabilze training : from huggingface tutorials\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      discounted_return = (discounted_return - discounted_return.mean()) / (discounted_return.std() + eps)\n",
        "      # action_log_tensor = torch.tensor(action_log)\n",
        "      self._update_agent(action_log,discounted_return)\n",
        "\n",
        "\n",
        "      if episode != 0 and episode % 100 == 0:\n",
        "        print(f\"Episode {episode}\\tAverage Score: {np.mean(scores_deque)}\")\n",
        "\n",
        "  def evaluate(self, test_episodes: int):\n",
        "    scores = []\n",
        "    for episode in range(test_episodes):\n",
        "      state = self._env.reset()\n",
        "      episode_rewards = []\n",
        "      done = False\n",
        "      time_step = 0\n",
        "      while not done:\n",
        "        time_step += 1\n",
        "        action, _ = self._policy.act(state)\n",
        "        next_state, reward, termi, trunc, _ = self._env.step(action)\n",
        "        episode_rewards.append(reward)\n",
        "        done = termi or trunc\n",
        "        if np.sum(episode_rewards) > 510:\n",
        "          print(f\"Test No: {episode} Environment Solved in {time_step} \\t Score : {np.sum(episode_rewards)}\")\n",
        "          done = True\n",
        "\n",
        "        state = next_state\n",
        "      print(f\"Logger Data: Episode {episode}\\t Score: {np.sum(episode_rewards)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e7dWOaYASE4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6729fe52-637f-498c-96f1-514477d0650f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Agent Test Area"
      ],
      "metadata": {
        "id": "uU3lKs3CU_oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write me a code to test my agent reinforce class generate rraj method\n",
        "import gym\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "\n",
        "agent = AgentReinforce(env,1e-2,1.0)\n",
        "#agent.train(1000)"
      ],
      "metadata": {
        "id": "pPDtz0KISE7q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Before training\n",
        "agent.evaluate(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jADvzG77CxzE",
        "outputId": "ebaae691-821d-4d0b-a382-f3069d94fa01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logger Data: Episode 0\t Score: 14.0\n",
            "Logger Data: Episode 1\t Score: 12.0\n",
            "Logger Data: Episode 2\t Score: 9.0\n",
            "Logger Data: Episode 3\t Score: 17.0\n",
            "Logger Data: Episode 4\t Score: 13.0\n",
            "Logger Data: Episode 5\t Score: 20.0\n",
            "Logger Data: Episode 6\t Score: 21.0\n",
            "Logger Data: Episode 7\t Score: 12.0\n",
            "Logger Data: Episode 8\t Score: 33.0\n",
            "Logger Data: Episode 9\t Score: 21.0\n",
            "Logger Data: Episode 10\t Score: 17.0\n",
            "Logger Data: Episode 11\t Score: 23.0\n",
            "Logger Data: Episode 12\t Score: 20.0\n",
            "Logger Data: Episode 13\t Score: 11.0\n",
            "Logger Data: Episode 14\t Score: 14.0\n",
            "Logger Data: Episode 15\t Score: 10.0\n",
            "Logger Data: Episode 16\t Score: 18.0\n",
            "Logger Data: Episode 17\t Score: 18.0\n",
            "Logger Data: Episode 18\t Score: 33.0\n",
            "Logger Data: Episode 19\t Score: 17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZhKgUSgHiBu",
        "outputId": "56689515-fac6-47f4-d426-3e8429e20cbd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 61.64\n",
            "Episode 200\tAverage Score: 222.18\n",
            "Episode 300\tAverage Score: 444.29\n",
            "Episode 400\tAverage Score: 501.0\n",
            "Episode 500\tAverage Score: 488.58\n",
            "Episode 600\tAverage Score: 277.67\n",
            "Episode 700\tAverage Score: 102.13\n",
            "Episode 800\tAverage Score: 161.12\n",
            "Episode 900\tAverage Score: 460.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.evaluate(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_bJm_XZHp7q",
        "outputId": "f3ce2a00-6af3-4522-b468-ff952100266e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logger Data: Episode 0\t Score: 500.0\n",
            "Logger Data: Episode 1\t Score: 500.0\n",
            "Logger Data: Episode 2\t Score: 500.0\n",
            "Logger Data: Episode 3\t Score: 147.0\n",
            "Logger Data: Episode 4\t Score: 500.0\n",
            "Logger Data: Episode 5\t Score: 137.0\n",
            "Logger Data: Episode 6\t Score: 500.0\n",
            "Logger Data: Episode 7\t Score: 119.0\n",
            "Logger Data: Episode 8\t Score: 279.0\n",
            "Logger Data: Episode 9\t Score: 500.0\n",
            "Logger Data: Episode 10\t Score: 500.0\n",
            "Logger Data: Episode 11\t Score: 500.0\n",
            "Logger Data: Episode 12\t Score: 500.0\n",
            "Logger Data: Episode 13\t Score: 500.0\n",
            "Logger Data: Episode 14\t Score: 500.0\n",
            "Logger Data: Episode 15\t Score: 500.0\n",
            "Logger Data: Episode 16\t Score: 500.0\n",
            "Logger Data: Episode 17\t Score: 143.0\n",
            "Logger Data: Episode 18\t Score: 500.0\n",
            "Logger Data: Episode 19\t Score: 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGNkC-dtHqJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cartpole_hyperparameters = {\n",
        "#     \"h_size\": 16,\n",
        "#     \"n_training_episodes\": 1000,\n",
        "#     \"n_evaluation_episodes\": 10,\n",
        "#     \"max_t\": 1000,\n",
        "#     \"gamma\": 1.0,\n",
        "#     \"lr\": 1e-2,\n",
        "#     \"env_id\": env_id,\n",
        "#     \"state_space\": s_size,\n",
        "#     \"action_space\": a_size,\n",
        "# }"
      ],
      "metadata": {
        "id": "nWekz4jqEod5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policy_loss = []\n",
        "# for log_prob, disc_return in zip(log_probs, discount_return):\n",
        "#   policy_loss.append(-log_prob * disc_return)\n",
        "#   print(policy_loss)\n",
        "#policy_loss = torch.cat(policy_loss).sum()\n"
      ],
      "metadata": {
        "id": "nQAXeC-kzzRA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}